{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PartOfSpeechClassifier.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyN1t2TVE86ZD+LSU+WbAwRe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/footprinthere/DeepLearningWithTensorflow/blob/main/PartOfSpeechClassifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsarYYCDQLV8"
      },
      "source": [
        "# Part of Speech Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYXEZxwdQPAJ"
      },
      "source": [
        "### 필요한 모듈 import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyTiG13bPkc6"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "from pprint import pprint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vV13UGs7QmhF"
      },
      "source": [
        "### 데이터 전처리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5d68CiNQvFK"
      },
      "source": [
        "입력으로 사용할 문장 4개와, 각 단어의 품사 정보를 포함한 label 데이터이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJ5Iyl_7QYwR"
      },
      "source": [
        "# example data\n",
        "sentences = [['I', 'feel', 'hungry'],\n",
        "     ['tensorflow', 'is', 'very', 'difficult'],\n",
        "     ['tensorflow', 'is', 'a', 'framework', 'for', 'deep', 'learning'],\n",
        "     ['tensorflow', 'is', 'very', 'fast', 'changing']]\n",
        "pos = [['pronoun', 'verb', 'adjective'],\n",
        "     ['noun', 'verb', 'adverb', 'adjective'],\n",
        "     ['noun', 'verb', 'determiner', 'noun', 'preposition', 'adjective', 'noun'],\n",
        "     ['noun', 'verb', 'adverb', 'adjective', 'verb']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YizsSus5RgFm"
      },
      "source": [
        "token을 정수 형태의 index로 변환하기 위한 dictionary를 생성한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGxTBjt2Q3pS"
      },
      "source": [
        "# create token dictionary for words\n",
        "word_list = ['<pad>'] + sorted(set(sum(sentences, [])))\n",
        "word2idx = {word : idx for idx, word in enumerate(word_list)}\n",
        "\n",
        "# create token dictionary for pos\n",
        "pos_list = ['<pad>'] + sorted(set(sum(pos, [])))\n",
        "pos2idx = {pos : idx for idx, pos in enumerate(pos_list)}\n",
        "idx2pos = {idx : pos for idx, pos in enumerate(pos_list)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HD-r8KDKSx70"
      },
      "source": [
        "생성한 dictionary를 이용해 token을 index로 변환하고, 여기에 padding을 수행해 길이를 10으로 맞춰준다.  \n",
        "또한 masking을 수행하기 위한 배열을 추가로 생성한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YGBCFT-RZjj"
      },
      "source": [
        "# convert sequence of tokens to sequence of indices\n",
        "x_data = list(map(lambda sentence : [word2idx.get(token) for token in sentence], sentences))\n",
        "x_data_len = list(map(lambda sentence : len(sentence), sentences))\n",
        "y_data = list(map(lambda sentence : [pos2idx.get(token) for token in sentence], pos))\n",
        "\n",
        "# padding\n",
        "maxlen = 10\n",
        "x_data = keras.preprocessing.sequence.pad_sequences(\n",
        "    x_data, maxlen=maxlen, padding='post', truncating='post'\n",
        ")\n",
        "x_data_mask = ((x_data != 0) * 1).astype(np.float32)\n",
        "y_data = keras.preprocessing.sequence.pad_sequences(\n",
        "    y_data, maxlen=maxlen, padding='post', truncating='post'\n",
        ")\n",
        "\n",
        "print(x_data)\n",
        "print(x_data_mask)\n",
        "print(y_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_0oAUgNTmKW"
      },
      "source": [
        "위에서 가공한 데이터를 기반으로 Dataset을 생성한다.  \n",
        "여기에는 sequence loss 계산에 사용하기 위한, padding 이전의 길이 정보가 포함된다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8lEdmiTSh9W"
      },
      "source": [
        "# generate data pipeline\n",
        "batch_size = 2\n",
        "dataset = tf.data.Dataset.from_tensor_slices((x_data, y_data, x_data_len)).\\\n",
        "    shuffle(buffer_size=4).batch(batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouLkFq3HUEs5"
      },
      "source": [
        "### 모델 생성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhg4N8riUQ_i"
      },
      "source": [
        "학습에 활용할 many-to-many RNN model을 생성한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCXkCF2tUD7E"
      },
      "source": [
        "# create RNN model\n",
        "num_classes = len(pos2idx)\n",
        "hidden_size = 10\n",
        "dim = len(word2idx)\n",
        "one_hot = np.eye(dim)\n",
        "\n",
        "model = keras.Sequential()\n",
        "model.add(layers.Embedding(\n",
        "    input_dim=dim, output_dim=dim, input_length=maxlen,\n",
        "    trainable=False, mask_zero=True,\n",
        "    embeddings_initializer=keras.initializers.Constant(one_hot)\n",
        "))\n",
        "model.add(layers.SimpleRNN(units=hidden_size, return_sequences=True))\n",
        "model.add(layers.TimeDistributed(layers.Dense(units=num_classes)))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaEWY5aHVByJ"
      },
      "source": [
        "### 모델 학습 준비"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNd5s5UJVFQu"
      },
      "source": [
        "모델의 loss를 계산하는 함수를 정의한다.  \n",
        "* tf.sequence_mask(length, maxlen)을 활용해 masking에 사용할 배열을 생성할 수 있다.\n",
        "* 미리 계산해둔 x_len 정보를 cast 한 배열 valid_len을 선언한다.\n",
        "* sparse_categorical_crossentropy()를 사용해 loss를 계산하고, 여기에 mask를 곱해 pad token에 대해 계산된 loss를 제외한다.\n",
        "* 그 결과를 평균해 sequence loss를 계산하고, 이것들을 다시 평균해 최종 loss를 구한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4rttXgrUBOL"
      },
      "source": [
        "# define loss function\n",
        "def loss_func(model, x, y, x_len, maxlen):\n",
        "    mask = tf.sequence_mask(x_len, maxlen, dtype=tf.float32)\n",
        "    valid_len = tf.cast(x_len, dtype=tf.float32)\n",
        "    sequence_loss = keras.losses.sparse_categorical_crossentropy(\n",
        "        y_true=y, y_pred=model(x), from_logits=True\n",
        "    ) * mask\n",
        "    sequence_loss = tf.reduce_sum(sequence_loss, axis=-1) / valid_len\n",
        "    return tf.reduce_mean(sequence_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buhW_DC1XLum"
      },
      "source": [
        "학습에 활용할 gradient를 계산하는 함수를 정의한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yi7vWRGcWV3N"
      },
      "source": [
        "# define gradient function\n",
        "def grad_func(model, x, y, x_len, maxlen):\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss = loss_func(model, x, y, x_len, maxlen)\n",
        "    return tape.gradient(loss, model.variables), loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOBtX96_Xbs7"
      },
      "source": [
        "학습에 활용할 hyper parameter와 optimizer를 생성한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7uhoAZrXaTd"
      },
      "source": [
        "lr = 0.1\n",
        "epochs = 30\n",
        "opt = tf.optimizers.Adam(learning_rate=lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amYgz3NZYEzK"
      },
      "source": [
        "### 모델 학습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Er6zYuHmYH-p"
      },
      "source": [
        "생성한 모델과 데이터셋을 바탕으로 학습을 수행한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VidgzR-wXoml"
      },
      "source": [
        "# train\n",
        "loss_hist = []\n",
        "\n",
        "for epoch in range(1, epochs+1):\n",
        "    avg_loss = 0\n",
        "    tr_step = 0\n",
        "\n",
        "    for x, y, x_len in dataset:\n",
        "        grads, loss = grad_func(model, x, y, x_len, maxlen)\n",
        "        opt.apply_gradients(zip(grads, model.variables))\n",
        "        avg_loss += loss\n",
        "        tr_step += 1\n",
        "    avg_loss /= tr_step\n",
        "    loss_hist.append(avg_loss)\n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "        print(\"epoch: {:3}, loss: {:.4f}\".format(epoch, avg_loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEYEJSvpY96u"
      },
      "source": [
        "주어진 label 데이터와 모델이 예측한 결과를 비교해본다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fw2Wgt0EYuTS"
      },
      "source": [
        "pred = model.predict(x_data)\n",
        "pred = np.argmax(pred, axis=-1) * x_data_mask\n",
        "\n",
        "pprint(list(map(\n",
        "    lambda row : [idx2pos.get(elm) for elm in row],\n",
        "    pred.astype(np.int32).tolist()\n",
        ")), width=128)\n",
        "pprint(pos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeb7ieOPZuAD"
      },
      "source": [
        "학습 과정에서 loss가 어떻게 변화했는지 그래프로 확인해본다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJgo5uLwZUvS"
      },
      "source": [
        "plt.plot(loss_hist)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}