{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2021_07_14_part2_lab.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPOm64vz5fBcNOcDJSWY63Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/footprinthere/DeepLearningWithTensorflow/blob/main/2021_07_14_part2_lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3g7zwFPNR8Rk"
      },
      "source": [
        "# Review : NN on MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yy3-lpZCRd1I"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUV8dMD0SB-V"
      },
      "source": [
        "# load dataset\n",
        "def load_mnist():\n",
        "    (train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.mnist.load_data()\n",
        "    # add channel dimmension\n",
        "    train_data = np.expand_dims(train_data, axis=-1)\n",
        "    test_data = np.expand_dims(test_data, axis=-1)\n",
        "    # normalize\n",
        "    train_data = train_data.astype(np.float32) / 255.0\n",
        "    test_data = test_data.astype(np.float32) / 255.0\n",
        "    # one-hot encoding\n",
        "    train_labels = tf.keras.utils.to_categorical(train_labels, 10)\n",
        "    test_labels = tf.keras.utils.to_categorical(test_labels, 10)\n",
        "\n",
        "    return train_data, train_labels, test_data, test_labels"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbZq35kUU59t"
      },
      "source": [
        "데이터를 불러오면서 정규화를 수행한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNUhYp4fU9tt"
      },
      "source": [
        "입력 데이터(train_data, test_data)의 마지막 차원은 channel dimmension으로 확보해두어야 한다. 이때 np.expand.dims()를 사용한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbrCQx7KVFKp"
      },
      "source": [
        "tf.keras.utils.to_categorical()을 사용하면 label 데이터를 one-hot 방식으로 변환할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kz_vVpjhTF8-"
      },
      "source": [
        "# create Dense object\n",
        "def dense(units, weight_init):\n",
        "    return tf.keras.layers.Dense(\n",
        "        units=units, activation='relu', use_bias=True, kernel_initializer=weight_init\n",
        "    )"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mk9qyTfnVOrX"
      },
      "source": [
        "tf.keras.layers.Dense() 객체를 쉽게 생성하기 위한 함수를 정의한다. activation function으로 ReLU를 사용하도록 설정한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWenhRPIURMI"
      },
      "source": [
        "# create sequential NN model\n",
        "def create_model(label_dim):\n",
        "    # sequential model with Flatten\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    # 2 hidden layers\n",
        "    weight_init = tf.keras.initializers.glorot_uniform()    # Xavier initializer\n",
        "    for i in range(2):\n",
        "        model.add(dense(256, weight_init))\n",
        "        model.add(tf.keras.layers.Dropout(rate=0.5))    # dropout\n",
        "    # output layer\n",
        "    model.add(dense(label_dim, weight_init))\n",
        "\n",
        "    return model"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfIK7n6UVWd5"
      },
      "source": [
        "Sequential 모델을 생성하는 함수를 정의한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDVc-5slVb6u"
      },
      "source": [
        "weight initializer로는 Xavier initializer(tf.keras.initializers.glorot_uniform())을 사용한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKAr8NtgVldr"
      },
      "source": [
        "tf.keras.layers.Dropout() 객체를 add 하여 rate=0.5의 dropout을 적용한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvBbb7KbVrAK"
      },
      "source": [
        "# loss function\n",
        "def loss_func(model, images, labels):\n",
        "    logits = model(images, training=True)\n",
        "    loss = tf.reduce_mean(\n",
        "        tf.keras.losses.categorical_crossentropy(y_true=labels, y_pred=logits, from_logits=True)\n",
        "    )\n",
        "    return loss"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzm7SQ6sWNnQ"
      },
      "source": [
        "loss를 계산하는 함수를 정의한다. cross-entropy 방식을 사용한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JY3k-e5XWNM9"
      },
      "source": [
        "# accuracy function\n",
        "def accuracy_func(model, images, labels):\n",
        "    logits = model(images, training=True)\n",
        "    prediction = tf.equal(tf.argmax(logits, -1), tf.argmax(labels, -1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(prediction, tf.float32))\n",
        "    return accuracy\n",
        "\n",
        "# gradient function\n",
        "def grad_func(model, images, labels):\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss = loss_func(model, images, labels)\n",
        "    return tape.gradient(loss, model.variables)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40O1u_DVYYqK"
      },
      "source": [
        "accuracy를 계산하는 함수와 gradient를 계산하는 함수를 각각 정의한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rd-CDDqDXXyv"
      },
      "source": [
        "\"\"\" Dataset \"\"\"\n",
        "train_x, train_y, test_x, test_y = load_mnist()\n",
        "\n",
        "\"\"\" Hyper-parameters \"\"\"\n",
        "learning_rate = 0.001\n",
        "batch_size = 126\n",
        "\n",
        "training_epochs = 1\n",
        "training_iterations = len(train_x) // batch_size\n",
        "\n",
        "label_dim = 10"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMtGwLLWYwiN"
      },
      "source": [
        "MNIST 데이터셋을 로드하고 모델의 hyper-parameter를 설정한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOb1u_PjYfyY"
      },
      "source": [
        "\"\"\" Graph Input using Dataset API \"\"\"\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_x, train_y)).\\\n",
        "    shuffle(buffer_size=100000).\\\n",
        "    prefetch(buffer_size=batch_size).\\\n",
        "    batch(batch_size, drop_remainder=True)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_x, test_y)).\\\n",
        "    shuffle(buffer_size=100000).\\\n",
        "    prefetch(buffer_size=len(test_x)).\\\n",
        "    batch(batch_size=len(test_x))"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpvqcTrpZ0nq"
      },
      "source": [
        "training set을 한 번에 batch_size만큼만 불러와서 처리할 수 있도록 batch()로 세팅한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Y-13yvHZ6jd"
      },
      "source": [
        "shuffle()은 데이터셋의 순서를 무작위로 섞어주며, prefetch()는 학습을 진행하는 동안 다음 단계를 위해 데이터의 일부를 대기시키도록 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "is50rbA6aEC_"
      },
      "source": [
        "test set은 한 번에 모두 불러와 사용할 수 있도록 세팅한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7plzaxdaHe_",
        "outputId": "e42c5dea-b25b-4ebb-ef32-b063f4421dc2"
      },
      "source": [
        "\"\"\" Model \"\"\"\n",
        "network = create_model(label_dim)\n",
        "\n",
        "\"\"\" Train \"\"\"\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "for epoch in range(training_epochs):\n",
        "    for idx, (train_input, train_label) in enumerate(train_dataset):\n",
        "        grads = grad_func(network, train_input, train_label)\n",
        "        optimizer.apply_gradients(zip(grads, network.variables))\n",
        "        # train loss and accuracy\n",
        "        train_loss = loss_func(network, train_input, train_label)\n",
        "        train_accuracy = accuracy_func(network, train_input ,train_label)\n",
        "        # test accuarcy\n",
        "        for test_input, test_label in test_dataset:\n",
        "            test_accuracy = accuracy_func(network, test_input, test_label)\n",
        "        # print result\n",
        "        print(\"epoch {:2}({:5}/{:5}) | loss {:.8f} | train accuracy {:.4f} | test accuracy {:.4f}\".\\\n",
        "              format(epoch, idx, training_iterations, train_loss, train_accuracy, test_accuracy))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch  0(    0/  476) | loss 2.30697656 | train accuracy 0.1190 | test accuracy 0.1245\n",
            "epoch  0(    1/  476) | loss 2.27788711 | train accuracy 0.1032 | test accuracy 0.1520\n",
            "epoch  0(    2/  476) | loss 2.21571159 | train accuracy 0.1111 | test accuracy 0.1770\n",
            "epoch  0(    3/  476) | loss 2.16436219 | train accuracy 0.2381 | test accuracy 0.2108\n",
            "epoch  0(    4/  476) | loss 2.16342974 | train accuracy 0.1825 | test accuracy 0.2360\n",
            "epoch  0(    5/  476) | loss 2.11502790 | train accuracy 0.2222 | test accuracy 0.2736\n",
            "epoch  0(    6/  476) | loss 2.09807229 | train accuracy 0.2937 | test accuracy 0.3073\n",
            "epoch  0(    7/  476) | loss 2.04900336 | train accuracy 0.3571 | test accuracy 0.3291\n",
            "epoch  0(    8/  476) | loss 2.03467417 | train accuracy 0.4365 | test accuracy 0.3642\n",
            "epoch  0(    9/  476) | loss 1.92839897 | train accuracy 0.4921 | test accuracy 0.3968\n",
            "epoch  0(   10/  476) | loss 1.94261265 | train accuracy 0.3889 | test accuracy 0.4232\n",
            "epoch  0(   11/  476) | loss 1.81530690 | train accuracy 0.4365 | test accuracy 0.4429\n",
            "epoch  0(   12/  476) | loss 1.70007181 | train accuracy 0.4921 | test accuracy 0.4639\n",
            "epoch  0(   13/  476) | loss 1.82019269 | train accuracy 0.3810 | test accuracy 0.4906\n",
            "epoch  0(   14/  476) | loss 1.68534410 | train accuracy 0.4444 | test accuracy 0.5103\n",
            "epoch  0(   15/  476) | loss 1.56489897 | train accuracy 0.5714 | test accuracy 0.5238\n",
            "epoch  0(   16/  476) | loss 1.56859779 | train accuracy 0.5952 | test accuracy 0.5357\n",
            "epoch  0(   17/  476) | loss 1.48689079 | train accuracy 0.5714 | test accuracy 0.5559\n",
            "epoch  0(   18/  476) | loss 1.42205501 | train accuracy 0.5556 | test accuracy 0.5637\n",
            "epoch  0(   19/  476) | loss 1.52299583 | train accuracy 0.5476 | test accuracy 0.5760\n",
            "epoch  0(   20/  476) | loss 1.47883177 | train accuracy 0.5952 | test accuracy 0.5813\n",
            "epoch  0(   21/  476) | loss 1.22851288 | train accuracy 0.5952 | test accuracy 0.5971\n",
            "epoch  0(   22/  476) | loss 1.32554913 | train accuracy 0.6349 | test accuracy 0.5959\n",
            "epoch  0(   23/  476) | loss 1.25074518 | train accuracy 0.5952 | test accuracy 0.6188\n",
            "epoch  0(   24/  476) | loss 1.28955853 | train accuracy 0.6587 | test accuracy 0.6196\n",
            "epoch  0(   25/  476) | loss 1.18967485 | train accuracy 0.5873 | test accuracy 0.6211\n",
            "epoch  0(   26/  476) | loss 1.24765062 | train accuracy 0.6032 | test accuracy 0.6291\n",
            "epoch  0(   27/  476) | loss 1.19392085 | train accuracy 0.7063 | test accuracy 0.6527\n",
            "epoch  0(   28/  476) | loss 1.03749061 | train accuracy 0.6190 | test accuracy 0.6480\n",
            "epoch  0(   29/  476) | loss 0.98702592 | train accuracy 0.6905 | test accuracy 0.6614\n",
            "epoch  0(   30/  476) | loss 0.97391468 | train accuracy 0.6667 | test accuracy 0.6695\n",
            "epoch  0(   31/  476) | loss 0.95412296 | train accuracy 0.6825 | test accuracy 0.6733\n",
            "epoch  0(   32/  476) | loss 0.98511213 | train accuracy 0.6587 | test accuracy 0.6779\n",
            "epoch  0(   33/  476) | loss 0.93962455 | train accuracy 0.7143 | test accuracy 0.6921\n",
            "epoch  0(   34/  476) | loss 0.78156662 | train accuracy 0.6587 | test accuracy 0.6848\n",
            "epoch  0(   35/  476) | loss 0.99264359 | train accuracy 0.7143 | test accuracy 0.7057\n",
            "epoch  0(   36/  476) | loss 0.95989865 | train accuracy 0.6429 | test accuracy 0.7099\n",
            "epoch  0(   37/  476) | loss 0.90798700 | train accuracy 0.6905 | test accuracy 0.7125\n",
            "epoch  0(   38/  476) | loss 0.76838833 | train accuracy 0.7619 | test accuracy 0.7215\n",
            "epoch  0(   39/  476) | loss 0.83404988 | train accuracy 0.7460 | test accuracy 0.7313\n",
            "epoch  0(   40/  476) | loss 0.84043497 | train accuracy 0.6746 | test accuracy 0.7333\n",
            "epoch  0(   41/  476) | loss 0.74588323 | train accuracy 0.7857 | test accuracy 0.7398\n",
            "epoch  0(   42/  476) | loss 0.60402244 | train accuracy 0.7937 | test accuracy 0.7445\n",
            "epoch  0(   43/  476) | loss 0.87119621 | train accuracy 0.7460 | test accuracy 0.7429\n",
            "epoch  0(   44/  476) | loss 0.71761525 | train accuracy 0.7540 | test accuracy 0.7492\n",
            "epoch  0(   45/  476) | loss 0.80007684 | train accuracy 0.7381 | test accuracy 0.7524\n",
            "epoch  0(   46/  476) | loss 0.55564183 | train accuracy 0.7857 | test accuracy 0.7572\n",
            "epoch  0(   47/  476) | loss 0.86107564 | train accuracy 0.7778 | test accuracy 0.7623\n",
            "epoch  0(   48/  476) | loss 0.74097222 | train accuracy 0.7222 | test accuracy 0.7578\n",
            "epoch  0(   49/  476) | loss 0.76754624 | train accuracy 0.7381 | test accuracy 0.7672\n",
            "epoch  0(   50/  476) | loss 0.94692934 | train accuracy 0.6429 | test accuracy 0.7704\n",
            "epoch  0(   51/  476) | loss 0.73039269 | train accuracy 0.7619 | test accuracy 0.7730\n",
            "epoch  0(   52/  476) | loss 0.64401609 | train accuracy 0.8016 | test accuracy 0.7791\n",
            "epoch  0(   53/  476) | loss 0.52864563 | train accuracy 0.7619 | test accuracy 0.7768\n",
            "epoch  0(   54/  476) | loss 0.75615412 | train accuracy 0.7222 | test accuracy 0.7865\n",
            "epoch  0(   55/  476) | loss 0.69613636 | train accuracy 0.8095 | test accuracy 0.7763\n",
            "epoch  0(   56/  476) | loss 0.68477023 | train accuracy 0.7937 | test accuracy 0.7932\n",
            "epoch  0(   57/  476) | loss 0.55925041 | train accuracy 0.7857 | test accuracy 0.7892\n",
            "epoch  0(   58/  476) | loss 0.70688319 | train accuracy 0.7619 | test accuracy 0.7915\n",
            "epoch  0(   59/  476) | loss 0.63235867 | train accuracy 0.7778 | test accuracy 0.7869\n",
            "epoch  0(   60/  476) | loss 0.51131445 | train accuracy 0.8254 | test accuracy 0.7954\n",
            "epoch  0(   61/  476) | loss 0.69341964 | train accuracy 0.8016 | test accuracy 0.7957\n",
            "epoch  0(   62/  476) | loss 0.63970470 | train accuracy 0.8175 | test accuracy 0.7986\n",
            "epoch  0(   63/  476) | loss 0.67446512 | train accuracy 0.8095 | test accuracy 0.8003\n",
            "epoch  0(   64/  476) | loss 0.84765232 | train accuracy 0.7063 | test accuracy 0.8025\n",
            "epoch  0(   65/  476) | loss 0.61117822 | train accuracy 0.8175 | test accuracy 0.8013\n",
            "epoch  0(   66/  476) | loss 0.63111591 | train accuracy 0.7698 | test accuracy 0.8050\n",
            "epoch  0(   67/  476) | loss 0.62916028 | train accuracy 0.7937 | test accuracy 0.8066\n",
            "epoch  0(   68/  476) | loss 0.59840733 | train accuracy 0.8492 | test accuracy 0.8091\n",
            "epoch  0(   69/  476) | loss 0.61081094 | train accuracy 0.8333 | test accuracy 0.8090\n",
            "epoch  0(   70/  476) | loss 0.53146511 | train accuracy 0.8254 | test accuracy 0.8139\n",
            "epoch  0(   71/  476) | loss 0.76580560 | train accuracy 0.7540 | test accuracy 0.8127\n",
            "epoch  0(   72/  476) | loss 0.48133886 | train accuracy 0.8333 | test accuracy 0.8197\n",
            "epoch  0(   73/  476) | loss 0.41412351 | train accuracy 0.8492 | test accuracy 0.8170\n",
            "epoch  0(   74/  476) | loss 0.54865372 | train accuracy 0.8254 | test accuracy 0.8216\n",
            "epoch  0(   75/  476) | loss 0.63371193 | train accuracy 0.7857 | test accuracy 0.8187\n",
            "epoch  0(   76/  476) | loss 0.59051549 | train accuracy 0.8175 | test accuracy 0.8222\n",
            "epoch  0(   77/  476) | loss 0.64119637 | train accuracy 0.7302 | test accuracy 0.8211\n",
            "epoch  0(   78/  476) | loss 0.68923557 | train accuracy 0.8492 | test accuracy 0.8214\n",
            "epoch  0(   79/  476) | loss 0.79307616 | train accuracy 0.7619 | test accuracy 0.8269\n",
            "epoch  0(   80/  476) | loss 0.52021003 | train accuracy 0.8413 | test accuracy 0.8331\n",
            "epoch  0(   81/  476) | loss 0.48446235 | train accuracy 0.8810 | test accuracy 0.8310\n",
            "epoch  0(   82/  476) | loss 0.50614184 | train accuracy 0.8254 | test accuracy 0.8261\n",
            "epoch  0(   83/  476) | loss 0.57252854 | train accuracy 0.8492 | test accuracy 0.8331\n",
            "epoch  0(   84/  476) | loss 0.49022597 | train accuracy 0.8095 | test accuracy 0.8297\n",
            "epoch  0(   85/  476) | loss 0.55508488 | train accuracy 0.8810 | test accuracy 0.8302\n",
            "epoch  0(   86/  476) | loss 0.57595491 | train accuracy 0.8175 | test accuracy 0.8305\n",
            "epoch  0(   87/  476) | loss 0.49562985 | train accuracy 0.8333 | test accuracy 0.8385\n",
            "epoch  0(   88/  476) | loss 0.52342230 | train accuracy 0.8651 | test accuracy 0.8390\n",
            "epoch  0(   89/  476) | loss 0.46933219 | train accuracy 0.8254 | test accuracy 0.8393\n",
            "epoch  0(   90/  476) | loss 0.57634258 | train accuracy 0.8333 | test accuracy 0.8420\n",
            "epoch  0(   91/  476) | loss 0.51694751 | train accuracy 0.8730 | test accuracy 0.8450\n",
            "epoch  0(   92/  476) | loss 0.52823865 | train accuracy 0.8492 | test accuracy 0.8384\n",
            "epoch  0(   93/  476) | loss 0.58566988 | train accuracy 0.7937 | test accuracy 0.8445\n",
            "epoch  0(   94/  476) | loss 0.48392203 | train accuracy 0.8254 | test accuracy 0.8457\n",
            "epoch  0(   95/  476) | loss 0.48556170 | train accuracy 0.8413 | test accuracy 0.8439\n",
            "epoch  0(   96/  476) | loss 0.51026261 | train accuracy 0.8254 | test accuracy 0.8442\n",
            "epoch  0(   97/  476) | loss 0.54460597 | train accuracy 0.8095 | test accuracy 0.8481\n",
            "epoch  0(   98/  476) | loss 0.36677516 | train accuracy 0.8810 | test accuracy 0.8501\n",
            "epoch  0(   99/  476) | loss 0.40482011 | train accuracy 0.8810 | test accuracy 0.8494\n",
            "epoch  0(  100/  476) | loss 0.36358696 | train accuracy 0.8651 | test accuracy 0.8491\n",
            "epoch  0(  101/  476) | loss 0.27032110 | train accuracy 0.9127 | test accuracy 0.8492\n",
            "epoch  0(  102/  476) | loss 0.47604567 | train accuracy 0.9048 | test accuracy 0.8487\n",
            "epoch  0(  103/  476) | loss 0.47186291 | train accuracy 0.8254 | test accuracy 0.8495\n",
            "epoch  0(  104/  476) | loss 0.50113815 | train accuracy 0.8571 | test accuracy 0.8493\n",
            "epoch  0(  105/  476) | loss 0.37981507 | train accuracy 0.8810 | test accuracy 0.8535\n",
            "epoch  0(  106/  476) | loss 0.31799331 | train accuracy 0.8810 | test accuracy 0.8513\n",
            "epoch  0(  107/  476) | loss 0.37728101 | train accuracy 0.8571 | test accuracy 0.8527\n",
            "epoch  0(  108/  476) | loss 0.40825003 | train accuracy 0.8651 | test accuracy 0.8566\n",
            "epoch  0(  109/  476) | loss 0.44647354 | train accuracy 0.8730 | test accuracy 0.8605\n",
            "epoch  0(  110/  476) | loss 0.45681396 | train accuracy 0.8016 | test accuracy 0.8605\n",
            "epoch  0(  111/  476) | loss 0.64846188 | train accuracy 0.8095 | test accuracy 0.8604\n",
            "epoch  0(  112/  476) | loss 0.47894603 | train accuracy 0.8492 | test accuracy 0.8591\n",
            "epoch  0(  113/  476) | loss 0.47010735 | train accuracy 0.8175 | test accuracy 0.8563\n",
            "epoch  0(  114/  476) | loss 0.44118109 | train accuracy 0.8571 | test accuracy 0.8557\n",
            "epoch  0(  115/  476) | loss 0.36035344 | train accuracy 0.8810 | test accuracy 0.8586\n",
            "epoch  0(  116/  476) | loss 0.40507784 | train accuracy 0.8571 | test accuracy 0.8616\n",
            "epoch  0(  117/  476) | loss 0.56268823 | train accuracy 0.8571 | test accuracy 0.8553\n",
            "epoch  0(  118/  476) | loss 0.65478420 | train accuracy 0.8175 | test accuracy 0.8605\n",
            "epoch  0(  119/  476) | loss 0.42147988 | train accuracy 0.8730 | test accuracy 0.8615\n",
            "epoch  0(  120/  476) | loss 0.49791020 | train accuracy 0.8651 | test accuracy 0.8638\n",
            "epoch  0(  121/  476) | loss 0.37700015 | train accuracy 0.8810 | test accuracy 0.8642\n",
            "epoch  0(  122/  476) | loss 0.41739538 | train accuracy 0.8730 | test accuracy 0.8647\n",
            "epoch  0(  123/  476) | loss 0.51630443 | train accuracy 0.8651 | test accuracy 0.8700\n",
            "epoch  0(  124/  476) | loss 0.30572441 | train accuracy 0.9286 | test accuracy 0.8614\n",
            "epoch  0(  125/  476) | loss 0.52697957 | train accuracy 0.8175 | test accuracy 0.8660\n",
            "epoch  0(  126/  476) | loss 0.38284084 | train accuracy 0.8810 | test accuracy 0.8677\n",
            "epoch  0(  127/  476) | loss 0.42971614 | train accuracy 0.8968 | test accuracy 0.8647\n",
            "epoch  0(  128/  476) | loss 0.45087168 | train accuracy 0.8571 | test accuracy 0.8677\n",
            "epoch  0(  129/  476) | loss 0.49714079 | train accuracy 0.8492 | test accuracy 0.8630\n",
            "epoch  0(  130/  476) | loss 0.52798748 | train accuracy 0.8413 | test accuracy 0.8675\n",
            "epoch  0(  131/  476) | loss 0.60915577 | train accuracy 0.8810 | test accuracy 0.8652\n",
            "epoch  0(  132/  476) | loss 0.49224374 | train accuracy 0.8492 | test accuracy 0.8633\n",
            "epoch  0(  133/  476) | loss 0.44276193 | train accuracy 0.8889 | test accuracy 0.8662\n",
            "epoch  0(  134/  476) | loss 0.42246035 | train accuracy 0.9365 | test accuracy 0.8674\n",
            "epoch  0(  135/  476) | loss 0.32110581 | train accuracy 0.9127 | test accuracy 0.8647\n",
            "epoch  0(  136/  476) | loss 0.38580027 | train accuracy 0.8889 | test accuracy 0.8658\n",
            "epoch  0(  137/  476) | loss 0.57122797 | train accuracy 0.8095 | test accuracy 0.8629\n",
            "epoch  0(  138/  476) | loss 0.39433151 | train accuracy 0.8492 | test accuracy 0.8650\n",
            "epoch  0(  139/  476) | loss 0.45198119 | train accuracy 0.8810 | test accuracy 0.8615\n",
            "epoch  0(  140/  476) | loss 0.47548258 | train accuracy 0.8571 | test accuracy 0.8672\n",
            "epoch  0(  141/  476) | loss 0.33311492 | train accuracy 0.9048 | test accuracy 0.8644\n",
            "epoch  0(  142/  476) | loss 0.45201471 | train accuracy 0.8333 | test accuracy 0.8689\n",
            "epoch  0(  143/  476) | loss 0.49207899 | train accuracy 0.8492 | test accuracy 0.8738\n",
            "epoch  0(  144/  476) | loss 0.45926645 | train accuracy 0.8254 | test accuracy 0.8713\n",
            "epoch  0(  145/  476) | loss 0.38831946 | train accuracy 0.8571 | test accuracy 0.8731\n",
            "epoch  0(  146/  476) | loss 0.53817779 | train accuracy 0.8333 | test accuracy 0.8727\n",
            "epoch  0(  147/  476) | loss 0.40250668 | train accuracy 0.8413 | test accuracy 0.8715\n",
            "epoch  0(  148/  476) | loss 0.42466703 | train accuracy 0.8730 | test accuracy 0.8731\n",
            "epoch  0(  149/  476) | loss 0.43718868 | train accuracy 0.8730 | test accuracy 0.8745\n",
            "epoch  0(  150/  476) | loss 0.26509613 | train accuracy 0.8968 | test accuracy 0.8730\n",
            "epoch  0(  151/  476) | loss 0.53376520 | train accuracy 0.8254 | test accuracy 0.8721\n",
            "epoch  0(  152/  476) | loss 0.39705306 | train accuracy 0.9206 | test accuracy 0.8710\n",
            "epoch  0(  153/  476) | loss 0.36948138 | train accuracy 0.8889 | test accuracy 0.8806\n",
            "epoch  0(  154/  476) | loss 0.47935137 | train accuracy 0.8571 | test accuracy 0.8729\n",
            "epoch  0(  155/  476) | loss 0.49818167 | train accuracy 0.8730 | test accuracy 0.8745\n",
            "epoch  0(  156/  476) | loss 0.28075671 | train accuracy 0.9127 | test accuracy 0.8793\n",
            "epoch  0(  157/  476) | loss 0.47482982 | train accuracy 0.8968 | test accuracy 0.8812\n",
            "epoch  0(  158/  476) | loss 0.55543596 | train accuracy 0.8492 | test accuracy 0.8835\n",
            "epoch  0(  159/  476) | loss 0.28459492 | train accuracy 0.9127 | test accuracy 0.8804\n",
            "epoch  0(  160/  476) | loss 0.34609973 | train accuracy 0.9127 | test accuracy 0.8830\n",
            "epoch  0(  161/  476) | loss 0.47992736 | train accuracy 0.8889 | test accuracy 0.8805\n",
            "epoch  0(  162/  476) | loss 0.42990023 | train accuracy 0.8810 | test accuracy 0.8823\n",
            "epoch  0(  163/  476) | loss 0.48567382 | train accuracy 0.8810 | test accuracy 0.8798\n",
            "epoch  0(  164/  476) | loss 0.46609160 | train accuracy 0.8730 | test accuracy 0.8808\n",
            "epoch  0(  165/  476) | loss 0.40750292 | train accuracy 0.8968 | test accuracy 0.8769\n",
            "epoch  0(  166/  476) | loss 0.46010023 | train accuracy 0.8413 | test accuracy 0.8781\n",
            "epoch  0(  167/  476) | loss 0.37436727 | train accuracy 0.8968 | test accuracy 0.8787\n",
            "epoch  0(  168/  476) | loss 0.38959548 | train accuracy 0.8810 | test accuracy 0.8809\n",
            "epoch  0(  169/  476) | loss 0.38166353 | train accuracy 0.8651 | test accuracy 0.8844\n",
            "epoch  0(  170/  476) | loss 0.32013518 | train accuracy 0.9206 | test accuracy 0.8839\n",
            "epoch  0(  171/  476) | loss 0.40397313 | train accuracy 0.9048 | test accuracy 0.8841\n",
            "epoch  0(  172/  476) | loss 0.41043976 | train accuracy 0.8413 | test accuracy 0.8847\n",
            "epoch  0(  173/  476) | loss 0.39578873 | train accuracy 0.8333 | test accuracy 0.8838\n",
            "epoch  0(  174/  476) | loss 0.43222767 | train accuracy 0.8492 | test accuracy 0.8873\n",
            "epoch  0(  175/  476) | loss 0.30303457 | train accuracy 0.9048 | test accuracy 0.8842\n",
            "epoch  0(  176/  476) | loss 0.46322566 | train accuracy 0.8254 | test accuracy 0.8834\n",
            "epoch  0(  177/  476) | loss 0.43970779 | train accuracy 0.9048 | test accuracy 0.8869\n",
            "epoch  0(  178/  476) | loss 0.42461440 | train accuracy 0.9048 | test accuracy 0.8856\n",
            "epoch  0(  179/  476) | loss 0.42903778 | train accuracy 0.8730 | test accuracy 0.8874\n",
            "epoch  0(  180/  476) | loss 0.47098213 | train accuracy 0.9048 | test accuracy 0.8875\n",
            "epoch  0(  181/  476) | loss 0.42673644 | train accuracy 0.8492 | test accuracy 0.8858\n",
            "epoch  0(  182/  476) | loss 0.44162673 | train accuracy 0.9048 | test accuracy 0.8875\n",
            "epoch  0(  183/  476) | loss 0.49431935 | train accuracy 0.8333 | test accuracy 0.8866\n",
            "epoch  0(  184/  476) | loss 0.31686866 | train accuracy 0.8889 | test accuracy 0.8887\n",
            "epoch  0(  185/  476) | loss 0.45833051 | train accuracy 0.8810 | test accuracy 0.8924\n",
            "epoch  0(  186/  476) | loss 0.44724807 | train accuracy 0.8968 | test accuracy 0.8908\n",
            "epoch  0(  187/  476) | loss 0.34083977 | train accuracy 0.9286 | test accuracy 0.8892\n",
            "epoch  0(  188/  476) | loss 0.60607815 | train accuracy 0.8254 | test accuracy 0.8886\n",
            "epoch  0(  189/  476) | loss 0.46213603 | train accuracy 0.8810 | test accuracy 0.8871\n",
            "epoch  0(  190/  476) | loss 0.48575109 | train accuracy 0.8651 | test accuracy 0.8895\n",
            "epoch  0(  191/  476) | loss 0.40564629 | train accuracy 0.8730 | test accuracy 0.8935\n",
            "epoch  0(  192/  476) | loss 0.47457227 | train accuracy 0.8333 | test accuracy 0.8880\n",
            "epoch  0(  193/  476) | loss 0.21882555 | train accuracy 0.9286 | test accuracy 0.8919\n",
            "epoch  0(  194/  476) | loss 0.28967777 | train accuracy 0.9206 | test accuracy 0.8896\n",
            "epoch  0(  195/  476) | loss 0.39124307 | train accuracy 0.8889 | test accuracy 0.8926\n",
            "epoch  0(  196/  476) | loss 0.33974394 | train accuracy 0.8810 | test accuracy 0.8908\n",
            "epoch  0(  197/  476) | loss 0.40636361 | train accuracy 0.9048 | test accuracy 0.8909\n",
            "epoch  0(  198/  476) | loss 0.31657842 | train accuracy 0.8968 | test accuracy 0.8925\n",
            "epoch  0(  199/  476) | loss 0.26037407 | train accuracy 0.9127 | test accuracy 0.8933\n",
            "epoch  0(  200/  476) | loss 0.28355613 | train accuracy 0.8810 | test accuracy 0.8907\n",
            "epoch  0(  201/  476) | loss 0.40659976 | train accuracy 0.8571 | test accuracy 0.8920\n",
            "epoch  0(  202/  476) | loss 0.40374979 | train accuracy 0.9048 | test accuracy 0.8980\n",
            "epoch  0(  203/  476) | loss 0.26794639 | train accuracy 0.9286 | test accuracy 0.8969\n",
            "epoch  0(  204/  476) | loss 0.32264426 | train accuracy 0.9127 | test accuracy 0.8975\n",
            "epoch  0(  205/  476) | loss 0.34803972 | train accuracy 0.8889 | test accuracy 0.8945\n",
            "epoch  0(  206/  476) | loss 0.40769336 | train accuracy 0.8730 | test accuracy 0.8944\n",
            "epoch  0(  207/  476) | loss 0.44354132 | train accuracy 0.8730 | test accuracy 0.8969\n",
            "epoch  0(  208/  476) | loss 0.51064432 | train accuracy 0.8968 | test accuracy 0.8996\n",
            "epoch  0(  209/  476) | loss 0.38002732 | train accuracy 0.8889 | test accuracy 0.8951\n",
            "epoch  0(  210/  476) | loss 0.25029695 | train accuracy 0.8968 | test accuracy 0.8990\n",
            "epoch  0(  211/  476) | loss 0.26342234 | train accuracy 0.8730 | test accuracy 0.8955\n",
            "epoch  0(  212/  476) | loss 0.34645110 | train accuracy 0.8889 | test accuracy 0.8957\n",
            "epoch  0(  213/  476) | loss 0.29457808 | train accuracy 0.8651 | test accuracy 0.8931\n",
            "epoch  0(  214/  476) | loss 0.34527233 | train accuracy 0.8810 | test accuracy 0.8966\n",
            "epoch  0(  215/  476) | loss 0.34500885 | train accuracy 0.8651 | test accuracy 0.8964\n",
            "epoch  0(  216/  476) | loss 0.34311101 | train accuracy 0.8968 | test accuracy 0.8968\n",
            "epoch  0(  217/  476) | loss 0.29213768 | train accuracy 0.8889 | test accuracy 0.8938\n",
            "epoch  0(  218/  476) | loss 0.37333968 | train accuracy 0.9127 | test accuracy 0.8937\n",
            "epoch  0(  219/  476) | loss 0.34378809 | train accuracy 0.9286 | test accuracy 0.8941\n",
            "epoch  0(  220/  476) | loss 0.35386014 | train accuracy 0.9127 | test accuracy 0.8979\n",
            "epoch  0(  221/  476) | loss 0.27270135 | train accuracy 0.9286 | test accuracy 0.8985\n",
            "epoch  0(  222/  476) | loss 0.37408558 | train accuracy 0.8810 | test accuracy 0.8969\n",
            "epoch  0(  223/  476) | loss 0.38856360 | train accuracy 0.9206 | test accuracy 0.8978\n",
            "epoch  0(  224/  476) | loss 0.40026787 | train accuracy 0.8889 | test accuracy 0.8982\n",
            "epoch  0(  225/  476) | loss 0.25752863 | train accuracy 0.8730 | test accuracy 0.8980\n",
            "epoch  0(  226/  476) | loss 0.29719296 | train accuracy 0.9206 | test accuracy 0.9005\n",
            "epoch  0(  227/  476) | loss 0.31158271 | train accuracy 0.9206 | test accuracy 0.8959\n",
            "epoch  0(  228/  476) | loss 0.43845674 | train accuracy 0.8810 | test accuracy 0.9012\n",
            "epoch  0(  229/  476) | loss 0.47763833 | train accuracy 0.8571 | test accuracy 0.8987\n",
            "epoch  0(  230/  476) | loss 0.31636357 | train accuracy 0.9127 | test accuracy 0.8985\n",
            "epoch  0(  231/  476) | loss 0.29299894 | train accuracy 0.8730 | test accuracy 0.8987\n",
            "epoch  0(  232/  476) | loss 0.30491784 | train accuracy 0.8968 | test accuracy 0.8983\n",
            "epoch  0(  233/  476) | loss 0.52426022 | train accuracy 0.8254 | test accuracy 0.9021\n",
            "epoch  0(  234/  476) | loss 0.31087235 | train accuracy 0.9524 | test accuracy 0.9013\n",
            "epoch  0(  235/  476) | loss 0.31538466 | train accuracy 0.9206 | test accuracy 0.8993\n",
            "epoch  0(  236/  476) | loss 0.30990547 | train accuracy 0.9444 | test accuracy 0.8981\n",
            "epoch  0(  237/  476) | loss 0.37330806 | train accuracy 0.8730 | test accuracy 0.9040\n",
            "epoch  0(  238/  476) | loss 0.35933447 | train accuracy 0.8810 | test accuracy 0.9046\n",
            "epoch  0(  239/  476) | loss 0.32141593 | train accuracy 0.9127 | test accuracy 0.9032\n",
            "epoch  0(  240/  476) | loss 0.28551823 | train accuracy 0.9206 | test accuracy 0.9046\n",
            "epoch  0(  241/  476) | loss 0.31759202 | train accuracy 0.8651 | test accuracy 0.9071\n",
            "epoch  0(  242/  476) | loss 0.36559156 | train accuracy 0.9286 | test accuracy 0.9029\n",
            "epoch  0(  243/  476) | loss 0.45076957 | train accuracy 0.8413 | test accuracy 0.9032\n",
            "epoch  0(  244/  476) | loss 0.23785314 | train accuracy 0.9206 | test accuracy 0.9047\n",
            "epoch  0(  245/  476) | loss 0.45305082 | train accuracy 0.8730 | test accuracy 0.9014\n",
            "epoch  0(  246/  476) | loss 0.35417217 | train accuracy 0.9127 | test accuracy 0.9024\n",
            "epoch  0(  247/  476) | loss 0.25467747 | train accuracy 0.9286 | test accuracy 0.9026\n",
            "epoch  0(  248/  476) | loss 0.40506956 | train accuracy 0.8651 | test accuracy 0.9029\n",
            "epoch  0(  249/  476) | loss 0.27861273 | train accuracy 0.9524 | test accuracy 0.8990\n",
            "epoch  0(  250/  476) | loss 0.40396875 | train accuracy 0.8730 | test accuracy 0.9024\n",
            "epoch  0(  251/  476) | loss 0.30683991 | train accuracy 0.9286 | test accuracy 0.9052\n",
            "epoch  0(  252/  476) | loss 0.34468889 | train accuracy 0.9524 | test accuracy 0.9030\n",
            "epoch  0(  253/  476) | loss 0.22518842 | train accuracy 0.8810 | test accuracy 0.9033\n",
            "epoch  0(  254/  476) | loss 0.21189201 | train accuracy 0.9524 | test accuracy 0.9051\n",
            "epoch  0(  255/  476) | loss 0.41133422 | train accuracy 0.9127 | test accuracy 0.9059\n",
            "epoch  0(  256/  476) | loss 0.29373723 | train accuracy 0.8651 | test accuracy 0.9047\n",
            "epoch  0(  257/  476) | loss 0.34290925 | train accuracy 0.9206 | test accuracy 0.9045\n",
            "epoch  0(  258/  476) | loss 0.17101465 | train accuracy 0.9365 | test accuracy 0.9058\n",
            "epoch  0(  259/  476) | loss 0.37501067 | train accuracy 0.8968 | test accuracy 0.9067\n",
            "epoch  0(  260/  476) | loss 0.29144222 | train accuracy 0.9444 | test accuracy 0.9023\n",
            "epoch  0(  261/  476) | loss 0.25856149 | train accuracy 0.9365 | test accuracy 0.9052\n",
            "epoch  0(  262/  476) | loss 0.27737695 | train accuracy 0.8968 | test accuracy 0.9055\n",
            "epoch  0(  263/  476) | loss 0.31618735 | train accuracy 0.8810 | test accuracy 0.9047\n",
            "epoch  0(  264/  476) | loss 0.24266374 | train accuracy 0.9286 | test accuracy 0.9045\n",
            "epoch  0(  265/  476) | loss 0.42184827 | train accuracy 0.8571 | test accuracy 0.9042\n",
            "epoch  0(  266/  476) | loss 0.26848391 | train accuracy 0.8730 | test accuracy 0.9064\n",
            "epoch  0(  267/  476) | loss 0.37450862 | train accuracy 0.9206 | test accuracy 0.9079\n",
            "epoch  0(  268/  476) | loss 0.35558289 | train accuracy 0.8810 | test accuracy 0.9060\n",
            "epoch  0(  269/  476) | loss 0.36491340 | train accuracy 0.8968 | test accuracy 0.9054\n",
            "epoch  0(  270/  476) | loss 0.35776740 | train accuracy 0.9127 | test accuracy 0.9063\n",
            "epoch  0(  271/  476) | loss 0.32414317 | train accuracy 0.9286 | test accuracy 0.9049\n",
            "epoch  0(  272/  476) | loss 0.35574141 | train accuracy 0.8889 | test accuracy 0.9073\n",
            "epoch  0(  273/  476) | loss 0.26373437 | train accuracy 0.8889 | test accuracy 0.9092\n",
            "epoch  0(  274/  476) | loss 0.35261855 | train accuracy 0.8968 | test accuracy 0.9074\n",
            "epoch  0(  275/  476) | loss 0.23340011 | train accuracy 0.8968 | test accuracy 0.9043\n",
            "epoch  0(  276/  476) | loss 0.29020399 | train accuracy 0.9127 | test accuracy 0.9045\n",
            "epoch  0(  277/  476) | loss 0.31065091 | train accuracy 0.8968 | test accuracy 0.9057\n",
            "epoch  0(  278/  476) | loss 0.41007212 | train accuracy 0.8730 | test accuracy 0.9038\n",
            "epoch  0(  279/  476) | loss 0.34587690 | train accuracy 0.8810 | test accuracy 0.9067\n",
            "epoch  0(  280/  476) | loss 0.26398367 | train accuracy 0.9524 | test accuracy 0.9075\n",
            "epoch  0(  281/  476) | loss 0.32429305 | train accuracy 0.8968 | test accuracy 0.9059\n",
            "epoch  0(  282/  476) | loss 0.24421589 | train accuracy 0.9206 | test accuracy 0.9064\n",
            "epoch  0(  283/  476) | loss 0.30162010 | train accuracy 0.8730 | test accuracy 0.9086\n",
            "epoch  0(  284/  476) | loss 0.31193131 | train accuracy 0.8889 | test accuracy 0.9111\n",
            "epoch  0(  285/  476) | loss 0.27872407 | train accuracy 0.9286 | test accuracy 0.9105\n",
            "epoch  0(  286/  476) | loss 0.23930101 | train accuracy 0.9206 | test accuracy 0.9070\n",
            "epoch  0(  287/  476) | loss 0.27229816 | train accuracy 0.8730 | test accuracy 0.9068\n",
            "epoch  0(  288/  476) | loss 0.30362830 | train accuracy 0.8968 | test accuracy 0.9100\n",
            "epoch  0(  289/  476) | loss 0.26599193 | train accuracy 0.9444 | test accuracy 0.9103\n",
            "epoch  0(  290/  476) | loss 0.36449131 | train accuracy 0.8810 | test accuracy 0.9081\n",
            "epoch  0(  291/  476) | loss 0.40265906 | train accuracy 0.8968 | test accuracy 0.9099\n",
            "epoch  0(  292/  476) | loss 0.39888266 | train accuracy 0.8651 | test accuracy 0.9092\n",
            "epoch  0(  293/  476) | loss 0.38631868 | train accuracy 0.8968 | test accuracy 0.9099\n",
            "epoch  0(  294/  476) | loss 0.18933009 | train accuracy 0.9365 | test accuracy 0.9119\n",
            "epoch  0(  295/  476) | loss 0.47473636 | train accuracy 0.9048 | test accuracy 0.9078\n",
            "epoch  0(  296/  476) | loss 0.31221288 | train accuracy 0.8810 | test accuracy 0.9123\n",
            "epoch  0(  297/  476) | loss 0.27458283 | train accuracy 0.9127 | test accuracy 0.9108\n",
            "epoch  0(  298/  476) | loss 0.38275531 | train accuracy 0.8968 | test accuracy 0.9102\n",
            "epoch  0(  299/  476) | loss 0.48976791 | train accuracy 0.9127 | test accuracy 0.9088\n",
            "epoch  0(  300/  476) | loss 0.19480613 | train accuracy 0.9048 | test accuracy 0.9089\n",
            "epoch  0(  301/  476) | loss 0.49681333 | train accuracy 0.8571 | test accuracy 0.9105\n",
            "epoch  0(  302/  476) | loss 0.25940862 | train accuracy 0.9127 | test accuracy 0.9136\n",
            "epoch  0(  303/  476) | loss 0.35834920 | train accuracy 0.9048 | test accuracy 0.9150\n",
            "epoch  0(  304/  476) | loss 0.25127888 | train accuracy 0.9286 | test accuracy 0.9124\n",
            "epoch  0(  305/  476) | loss 0.23937309 | train accuracy 0.9365 | test accuracy 0.9148\n",
            "epoch  0(  306/  476) | loss 0.23332472 | train accuracy 0.9127 | test accuracy 0.9084\n",
            "epoch  0(  307/  476) | loss 0.22873473 | train accuracy 0.9206 | test accuracy 0.9107\n",
            "epoch  0(  308/  476) | loss 0.26871863 | train accuracy 0.9127 | test accuracy 0.9093\n",
            "epoch  0(  309/  476) | loss 0.26199520 | train accuracy 0.9127 | test accuracy 0.9069\n",
            "epoch  0(  310/  476) | loss 0.17821775 | train accuracy 0.9048 | test accuracy 0.9077\n",
            "epoch  0(  311/  476) | loss 0.30721155 | train accuracy 0.9206 | test accuracy 0.9059\n",
            "epoch  0(  312/  476) | loss 0.39942360 | train accuracy 0.8413 | test accuracy 0.9067\n",
            "epoch  0(  313/  476) | loss 0.41294467 | train accuracy 0.8413 | test accuracy 0.9104\n",
            "epoch  0(  314/  476) | loss 0.27922276 | train accuracy 0.9127 | test accuracy 0.9118\n",
            "epoch  0(  315/  476) | loss 0.44193783 | train accuracy 0.8810 | test accuracy 0.9108\n",
            "epoch  0(  316/  476) | loss 0.29957956 | train accuracy 0.9048 | test accuracy 0.9107\n",
            "epoch  0(  317/  476) | loss 0.37285259 | train accuracy 0.9127 | test accuracy 0.9154\n",
            "epoch  0(  318/  476) | loss 0.34744048 | train accuracy 0.9206 | test accuracy 0.9139\n",
            "epoch  0(  319/  476) | loss 0.19279325 | train accuracy 0.9365 | test accuracy 0.9155\n",
            "epoch  0(  320/  476) | loss 0.33589864 | train accuracy 0.9206 | test accuracy 0.9121\n",
            "epoch  0(  321/  476) | loss 0.41471365 | train accuracy 0.8968 | test accuracy 0.9105\n",
            "epoch  0(  322/  476) | loss 0.40130681 | train accuracy 0.8968 | test accuracy 0.9119\n",
            "epoch  0(  323/  476) | loss 0.28353471 | train accuracy 0.9048 | test accuracy 0.9148\n",
            "epoch  0(  324/  476) | loss 0.27485779 | train accuracy 0.9206 | test accuracy 0.9156\n",
            "epoch  0(  325/  476) | loss 0.41136503 | train accuracy 0.8968 | test accuracy 0.9147\n",
            "epoch  0(  326/  476) | loss 0.24709055 | train accuracy 0.9286 | test accuracy 0.9168\n",
            "epoch  0(  327/  476) | loss 0.33153188 | train accuracy 0.9127 | test accuracy 0.9156\n",
            "epoch  0(  328/  476) | loss 0.35626152 | train accuracy 0.8651 | test accuracy 0.9155\n",
            "epoch  0(  329/  476) | loss 0.34823158 | train accuracy 0.9206 | test accuracy 0.9201\n",
            "epoch  0(  330/  476) | loss 0.22272488 | train accuracy 0.9286 | test accuracy 0.9169\n",
            "epoch  0(  331/  476) | loss 0.30925232 | train accuracy 0.9048 | test accuracy 0.9179\n",
            "epoch  0(  332/  476) | loss 0.26591352 | train accuracy 0.8968 | test accuracy 0.9157\n",
            "epoch  0(  333/  476) | loss 0.41605285 | train accuracy 0.8730 | test accuracy 0.9141\n",
            "epoch  0(  334/  476) | loss 0.38025391 | train accuracy 0.8730 | test accuracy 0.9154\n",
            "epoch  0(  335/  476) | loss 0.23243231 | train accuracy 0.9365 | test accuracy 0.9167\n",
            "epoch  0(  336/  476) | loss 0.35986668 | train accuracy 0.9048 | test accuracy 0.9150\n",
            "epoch  0(  337/  476) | loss 0.35774422 | train accuracy 0.8889 | test accuracy 0.9140\n",
            "epoch  0(  338/  476) | loss 0.25461352 | train accuracy 0.9127 | test accuracy 0.9153\n",
            "epoch  0(  339/  476) | loss 0.27280244 | train accuracy 0.9365 | test accuracy 0.9147\n",
            "epoch  0(  340/  476) | loss 0.24151133 | train accuracy 0.9365 | test accuracy 0.9164\n",
            "epoch  0(  341/  476) | loss 0.24690813 | train accuracy 0.9206 | test accuracy 0.9155\n",
            "epoch  0(  342/  476) | loss 0.36072984 | train accuracy 0.8571 | test accuracy 0.9095\n",
            "epoch  0(  343/  476) | loss 0.33464095 | train accuracy 0.9127 | test accuracy 0.9151\n",
            "epoch  0(  344/  476) | loss 0.27938586 | train accuracy 0.8889 | test accuracy 0.9149\n",
            "epoch  0(  345/  476) | loss 0.31512600 | train accuracy 0.8651 | test accuracy 0.9138\n",
            "epoch  0(  346/  476) | loss 0.23057166 | train accuracy 0.9762 | test accuracy 0.9152\n",
            "epoch  0(  347/  476) | loss 0.31295848 | train accuracy 0.9206 | test accuracy 0.9178\n",
            "epoch  0(  348/  476) | loss 0.27424353 | train accuracy 0.9048 | test accuracy 0.9146\n",
            "epoch  0(  349/  476) | loss 0.29615295 | train accuracy 0.9048 | test accuracy 0.9157\n",
            "epoch  0(  350/  476) | loss 0.22248648 | train accuracy 0.9444 | test accuracy 0.9175\n",
            "epoch  0(  351/  476) | loss 0.31576040 | train accuracy 0.9286 | test accuracy 0.9136\n",
            "epoch  0(  352/  476) | loss 0.19966258 | train accuracy 0.9127 | test accuracy 0.9163\n",
            "epoch  0(  353/  476) | loss 0.31922781 | train accuracy 0.9048 | test accuracy 0.9168\n",
            "epoch  0(  354/  476) | loss 0.38456017 | train accuracy 0.8413 | test accuracy 0.9215\n",
            "epoch  0(  355/  476) | loss 0.21704178 | train accuracy 0.9365 | test accuracy 0.9145\n",
            "epoch  0(  356/  476) | loss 0.21268985 | train accuracy 0.9127 | test accuracy 0.9157\n",
            "epoch  0(  357/  476) | loss 0.46017262 | train accuracy 0.9206 | test accuracy 0.9171\n",
            "epoch  0(  358/  476) | loss 0.31869441 | train accuracy 0.9048 | test accuracy 0.9216\n",
            "epoch  0(  359/  476) | loss 0.24824363 | train accuracy 0.9286 | test accuracy 0.9163\n",
            "epoch  0(  360/  476) | loss 0.15735689 | train accuracy 0.9444 | test accuracy 0.9133\n",
            "epoch  0(  361/  476) | loss 0.18173409 | train accuracy 0.9127 | test accuracy 0.9174\n",
            "epoch  0(  362/  476) | loss 0.27352637 | train accuracy 0.9286 | test accuracy 0.9154\n",
            "epoch  0(  363/  476) | loss 0.22685982 | train accuracy 0.9206 | test accuracy 0.9169\n",
            "epoch  0(  364/  476) | loss 0.45159081 | train accuracy 0.8889 | test accuracy 0.9173\n",
            "epoch  0(  365/  476) | loss 0.36937031 | train accuracy 0.9206 | test accuracy 0.9146\n",
            "epoch  0(  366/  476) | loss 0.21163508 | train accuracy 0.9365 | test accuracy 0.9199\n",
            "epoch  0(  367/  476) | loss 0.29643613 | train accuracy 0.8889 | test accuracy 0.9207\n",
            "epoch  0(  368/  476) | loss 0.36545053 | train accuracy 0.9365 | test accuracy 0.9225\n",
            "epoch  0(  369/  476) | loss 0.19075222 | train accuracy 0.9286 | test accuracy 0.9192\n",
            "epoch  0(  370/  476) | loss 0.29212683 | train accuracy 0.9286 | test accuracy 0.9193\n",
            "epoch  0(  371/  476) | loss 0.27259076 | train accuracy 0.9127 | test accuracy 0.9175\n",
            "epoch  0(  372/  476) | loss 0.19018847 | train accuracy 0.9127 | test accuracy 0.9173\n",
            "epoch  0(  373/  476) | loss 0.17656806 | train accuracy 0.9444 | test accuracy 0.9195\n",
            "epoch  0(  374/  476) | loss 0.29142141 | train accuracy 0.9206 | test accuracy 0.9190\n",
            "epoch  0(  375/  476) | loss 0.32752556 | train accuracy 0.9206 | test accuracy 0.9175\n",
            "epoch  0(  376/  476) | loss 0.17764941 | train accuracy 0.9365 | test accuracy 0.9169\n",
            "epoch  0(  377/  476) | loss 0.42841995 | train accuracy 0.9127 | test accuracy 0.9152\n",
            "epoch  0(  378/  476) | loss 0.25352043 | train accuracy 0.9365 | test accuracy 0.9165\n",
            "epoch  0(  379/  476) | loss 0.15270680 | train accuracy 0.9524 | test accuracy 0.9141\n",
            "epoch  0(  380/  476) | loss 0.20837747 | train accuracy 0.9206 | test accuracy 0.9155\n",
            "epoch  0(  381/  476) | loss 0.26219475 | train accuracy 0.9127 | test accuracy 0.9185\n",
            "epoch  0(  382/  476) | loss 0.24521874 | train accuracy 0.9127 | test accuracy 0.9159\n",
            "epoch  0(  383/  476) | loss 0.29254085 | train accuracy 0.9365 | test accuracy 0.9196\n",
            "epoch  0(  384/  476) | loss 0.40836737 | train accuracy 0.8730 | test accuracy 0.9182\n",
            "epoch  0(  385/  476) | loss 0.19936216 | train accuracy 0.9365 | test accuracy 0.9178\n",
            "epoch  0(  386/  476) | loss 0.23134768 | train accuracy 0.9127 | test accuracy 0.9184\n",
            "epoch  0(  387/  476) | loss 0.37510633 | train accuracy 0.8651 | test accuracy 0.9201\n",
            "epoch  0(  388/  476) | loss 0.19436499 | train accuracy 0.9683 | test accuracy 0.9223\n",
            "epoch  0(  389/  476) | loss 0.31409130 | train accuracy 0.9127 | test accuracy 0.9211\n",
            "epoch  0(  390/  476) | loss 0.39696875 | train accuracy 0.8571 | test accuracy 0.9191\n",
            "epoch  0(  391/  476) | loss 0.30138701 | train accuracy 0.8968 | test accuracy 0.9154\n",
            "epoch  0(  392/  476) | loss 0.30578506 | train accuracy 0.9048 | test accuracy 0.9196\n",
            "epoch  0(  393/  476) | loss 0.47594368 | train accuracy 0.8730 | test accuracy 0.9212\n",
            "epoch  0(  394/  476) | loss 0.23374607 | train accuracy 0.9127 | test accuracy 0.9191\n",
            "epoch  0(  395/  476) | loss 0.33407119 | train accuracy 0.9127 | test accuracy 0.9234\n",
            "epoch  0(  396/  476) | loss 0.20628221 | train accuracy 0.9603 | test accuracy 0.9185\n",
            "epoch  0(  397/  476) | loss 0.28422043 | train accuracy 0.9444 | test accuracy 0.9206\n",
            "epoch  0(  398/  476) | loss 0.24483007 | train accuracy 0.9127 | test accuracy 0.9185\n",
            "epoch  0(  399/  476) | loss 0.19668210 | train accuracy 0.9444 | test accuracy 0.9187\n",
            "epoch  0(  400/  476) | loss 0.28457755 | train accuracy 0.9444 | test accuracy 0.9189\n",
            "epoch  0(  401/  476) | loss 0.27596375 | train accuracy 0.9683 | test accuracy 0.9164\n",
            "epoch  0(  402/  476) | loss 0.22723435 | train accuracy 0.9127 | test accuracy 0.9194\n",
            "epoch  0(  403/  476) | loss 0.37647665 | train accuracy 0.8968 | test accuracy 0.9194\n",
            "epoch  0(  404/  476) | loss 0.30224192 | train accuracy 0.8810 | test accuracy 0.9171\n",
            "epoch  0(  405/  476) | loss 0.32897729 | train accuracy 0.9286 | test accuracy 0.9219\n",
            "epoch  0(  406/  476) | loss 0.30175400 | train accuracy 0.9048 | test accuracy 0.9230\n",
            "epoch  0(  407/  476) | loss 0.32643926 | train accuracy 0.9286 | test accuracy 0.9201\n",
            "epoch  0(  408/  476) | loss 0.28464183 | train accuracy 0.9206 | test accuracy 0.9223\n",
            "epoch  0(  409/  476) | loss 0.23172611 | train accuracy 0.9524 | test accuracy 0.9207\n",
            "epoch  0(  410/  476) | loss 0.29916966 | train accuracy 0.9365 | test accuracy 0.9233\n",
            "epoch  0(  411/  476) | loss 0.26674172 | train accuracy 0.8889 | test accuracy 0.9235\n",
            "epoch  0(  412/  476) | loss 0.26784137 | train accuracy 0.9365 | test accuracy 0.9220\n",
            "epoch  0(  413/  476) | loss 0.20818181 | train accuracy 0.9365 | test accuracy 0.9233\n",
            "epoch  0(  414/  476) | loss 0.36664590 | train accuracy 0.8968 | test accuracy 0.9252\n",
            "epoch  0(  415/  476) | loss 0.22427800 | train accuracy 0.9286 | test accuracy 0.9228\n",
            "epoch  0(  416/  476) | loss 0.27699623 | train accuracy 0.8968 | test accuracy 0.9215\n",
            "epoch  0(  417/  476) | loss 0.16021851 | train accuracy 0.9603 | test accuracy 0.9242\n",
            "epoch  0(  418/  476) | loss 0.17090513 | train accuracy 0.9524 | test accuracy 0.9241\n",
            "epoch  0(  419/  476) | loss 0.23426525 | train accuracy 0.9048 | test accuracy 0.9254\n",
            "epoch  0(  420/  476) | loss 0.32116085 | train accuracy 0.8810 | test accuracy 0.9252\n",
            "epoch  0(  421/  476) | loss 0.27361712 | train accuracy 0.9365 | test accuracy 0.9233\n",
            "epoch  0(  422/  476) | loss 0.23966207 | train accuracy 0.9524 | test accuracy 0.9198\n",
            "epoch  0(  423/  476) | loss 0.25345987 | train accuracy 0.9524 | test accuracy 0.9233\n",
            "epoch  0(  424/  476) | loss 0.24807127 | train accuracy 0.9444 | test accuracy 0.9226\n",
            "epoch  0(  425/  476) | loss 0.25369895 | train accuracy 0.9524 | test accuracy 0.9199\n",
            "epoch  0(  426/  476) | loss 0.42736572 | train accuracy 0.8810 | test accuracy 0.9208\n",
            "epoch  0(  427/  476) | loss 0.20839110 | train accuracy 0.9127 | test accuracy 0.9226\n",
            "epoch  0(  428/  476) | loss 0.20962642 | train accuracy 0.9206 | test accuracy 0.9207\n",
            "epoch  0(  429/  476) | loss 0.20153022 | train accuracy 0.8889 | test accuracy 0.9215\n",
            "epoch  0(  430/  476) | loss 0.37024337 | train accuracy 0.9286 | test accuracy 0.9214\n",
            "epoch  0(  431/  476) | loss 0.18220916 | train accuracy 0.9603 | test accuracy 0.9245\n",
            "epoch  0(  432/  476) | loss 0.38364133 | train accuracy 0.8730 | test accuracy 0.9254\n",
            "epoch  0(  433/  476) | loss 0.26103961 | train accuracy 0.8968 | test accuracy 0.9249\n",
            "epoch  0(  434/  476) | loss 0.24393216 | train accuracy 0.9206 | test accuracy 0.9243\n",
            "epoch  0(  435/  476) | loss 0.26977915 | train accuracy 0.9048 | test accuracy 0.9242\n",
            "epoch  0(  436/  476) | loss 0.30864716 | train accuracy 0.8968 | test accuracy 0.9233\n",
            "epoch  0(  437/  476) | loss 0.18780959 | train accuracy 0.9603 | test accuracy 0.9234\n",
            "epoch  0(  438/  476) | loss 0.24665144 | train accuracy 0.9127 | test accuracy 0.9237\n",
            "epoch  0(  439/  476) | loss 0.26015025 | train accuracy 0.9286 | test accuracy 0.9211\n",
            "epoch  0(  440/  476) | loss 0.19901025 | train accuracy 0.9444 | test accuracy 0.9213\n",
            "epoch  0(  441/  476) | loss 0.33514258 | train accuracy 0.9048 | test accuracy 0.9240\n",
            "epoch  0(  442/  476) | loss 0.22007853 | train accuracy 0.9286 | test accuracy 0.9263\n",
            "epoch  0(  443/  476) | loss 0.23276332 | train accuracy 0.9127 | test accuracy 0.9251\n",
            "epoch  0(  444/  476) | loss 0.27808028 | train accuracy 0.9206 | test accuracy 0.9261\n",
            "epoch  0(  445/  476) | loss 0.29188794 | train accuracy 0.9286 | test accuracy 0.9238\n",
            "epoch  0(  446/  476) | loss 0.26607972 | train accuracy 0.9127 | test accuracy 0.9236\n",
            "epoch  0(  447/  476) | loss 0.19077294 | train accuracy 0.9444 | test accuracy 0.9194\n",
            "epoch  0(  448/  476) | loss 0.17084366 | train accuracy 0.9444 | test accuracy 0.9212\n",
            "epoch  0(  449/  476) | loss 0.30139253 | train accuracy 0.9286 | test accuracy 0.9254\n",
            "epoch  0(  450/  476) | loss 0.29380959 | train accuracy 0.9048 | test accuracy 0.9240\n",
            "epoch  0(  451/  476) | loss 0.28421059 | train accuracy 0.9206 | test accuracy 0.9213\n",
            "epoch  0(  452/  476) | loss 0.29481816 | train accuracy 0.8889 | test accuracy 0.9245\n",
            "epoch  0(  453/  476) | loss 0.20970803 | train accuracy 0.9365 | test accuracy 0.9241\n",
            "epoch  0(  454/  476) | loss 0.13434139 | train accuracy 0.9683 | test accuracy 0.9210\n",
            "epoch  0(  455/  476) | loss 0.28178167 | train accuracy 0.9127 | test accuracy 0.9207\n",
            "epoch  0(  456/  476) | loss 0.31828818 | train accuracy 0.9048 | test accuracy 0.9237\n",
            "epoch  0(  457/  476) | loss 0.24734405 | train accuracy 0.9444 | test accuracy 0.9244\n",
            "epoch  0(  458/  476) | loss 0.24008168 | train accuracy 0.8889 | test accuracy 0.9281\n",
            "epoch  0(  459/  476) | loss 0.29035544 | train accuracy 0.9365 | test accuracy 0.9259\n",
            "epoch  0(  460/  476) | loss 0.23844345 | train accuracy 0.9127 | test accuracy 0.9252\n",
            "epoch  0(  461/  476) | loss 0.34090191 | train accuracy 0.8810 | test accuracy 0.9249\n",
            "epoch  0(  462/  476) | loss 0.27566156 | train accuracy 0.9444 | test accuracy 0.9253\n",
            "epoch  0(  463/  476) | loss 0.15893699 | train accuracy 0.9603 | test accuracy 0.9251\n",
            "epoch  0(  464/  476) | loss 0.16869563 | train accuracy 0.9365 | test accuracy 0.9234\n",
            "epoch  0(  465/  476) | loss 0.25281453 | train accuracy 0.9048 | test accuracy 0.9253\n",
            "epoch  0(  466/  476) | loss 0.19151673 | train accuracy 0.9603 | test accuracy 0.9247\n",
            "epoch  0(  467/  476) | loss 0.32470703 | train accuracy 0.9206 | test accuracy 0.9196\n",
            "epoch  0(  468/  476) | loss 0.14985524 | train accuracy 0.9524 | test accuracy 0.9214\n",
            "epoch  0(  469/  476) | loss 0.21980312 | train accuracy 0.9365 | test accuracy 0.9235\n",
            "epoch  0(  470/  476) | loss 0.33358836 | train accuracy 0.9206 | test accuracy 0.9219\n",
            "epoch  0(  471/  476) | loss 0.29192668 | train accuracy 0.8968 | test accuracy 0.9239\n",
            "epoch  0(  472/  476) | loss 0.14787196 | train accuracy 0.9365 | test accuracy 0.9233\n",
            "epoch  0(  473/  476) | loss 0.16033944 | train accuracy 0.9444 | test accuracy 0.9237\n",
            "epoch  0(  474/  476) | loss 0.16072781 | train accuracy 0.9286 | test accuracy 0.9259\n",
            "epoch  0(  475/  476) | loss 0.28198391 | train accuracy 0.9048 | test accuracy 0.9275\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HpFv5C0biNr"
      },
      "source": [
        "Adam optimizer를 이용해 학습한다."
      ]
    }
  ]
}